---
name: "A/B Test"
method_type: "ab-test"
evidence_links: []
confidence_level: "high"
date_validated: 2025-03-01
validator: "Data Team"
---

## How It Works

A/B testing is a controlled experiment where two or more variants of a product experience are shown to different groups of users at random. The team measures a predefined success metric, such as conversion rate, activation rate, or order frequency, and uses statistical analysis to determine whether the differences between groups are significant or just due to chance. Tests run until they reach a sufficient sample size to produce reliable results, which can take anywhere from a few days to several weeks depending on traffic volume and the size of the expected effect.

## When to Use It

A/B tests are ideal during the validation and building phases of customer development, when the team has a clear hypothesis and enough traffic to run a statistically valid experiment. They work best for evaluating specific, measurable changes, such as a new onboarding flow, a different pricing page, or a revised checkout experience. A/B tests are not well suited for exploratory research or for situations where the team does not yet know what to measure. They require a well-defined metric and a clear control experience to compare against.

## Confidence Level

A/B testing provides a high level of confidence because it directly measures the causal impact of a change on real user behavior in a controlled setting. When run correctly with proper sample sizes, randomization, and statistical rigor, A/B tests eliminate most of the guesswork involved in product decisions. However, confidence depends on test quality. Poorly designed tests with unclear metrics, insufficient sample sizes, or contaminated user groups can produce misleading results. The Data Team reviews all test designs before launch to ensure methodological soundness.
